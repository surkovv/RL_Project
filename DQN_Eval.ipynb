{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-19T10:53:39.828025Z",
     "start_time": "2025-05-19T10:53:39.104797Z"
    }
   },
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Agents.DQNAgent import DQNAgent"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T10:53:42.428439Z",
     "start_time": "2025-05-19T10:53:42.426015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ],
   "id": "d59845738eadfc3c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T10:54:33.963173Z",
     "start_time": "2025-05-19T10:54:33.960872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SEEDS = [42, 0, 5]\n",
    "\n",
    "#Hyperparameters\n",
    "gamma_list = [0.9, 0.95, 0.99]\n",
    "epsilon_decay_list = [0.99, 0.999]\n",
    "update_frequency_list = [1, 10, 20]\n",
    "batch_size_list = [32, 64]"
   ],
   "id": "6ba6aa79c917dbeb",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CartPole Environment",
   "id": "2afc64b3a42f2e92"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Hyperparameter Sensitivity (Learning Rate)",
   "id": "cd2158aa5dcdd8cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lrs = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "cartpole_results_lr = {}\n",
    "\n",
    "for lr in lrs:\n",
    "    all_rewards = []\n",
    "    for seed in SEEDS:\n",
    "        print(\"lr = \", lr, \"seed = \", seed)\n",
    "        set_seed(seed)\n",
    "        env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "        agent, rewards = DQNAgent.train_dqn(\n",
    "            env=env,\n",
    "            gamma=0.99,\n",
    "            batch_size=32,\n",
    "            epsilon=1.0,\n",
    "            epsilon_decay=0.99,\n",
    "            use_target_net=True,\n",
    "            constant_epsilon=False,\n",
    "            num_episodes=800,\n",
    "            target_update_freq=10,\n",
    "            lr=lr,\n",
    "            seed=seed\n",
    "        )\n",
    "\n",
    "        all_rewards.append(rewards)\n",
    "\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    avg_rewards = all_rewards.mean(axis=0)\n",
    "    cartpole_results_lr[f\"LR={lr}\"] = {\n",
    "        \"params\": {\n",
    "            \"lr\": lr,\n",
    "            \"gamma\": 0.99,\n",
    "            \"batch_size\": 64,\n",
    "            \"epsilon_decay\": 0.99,\n",
    "            \"target_update_freq\": 10\n",
    "        },\n",
    "        \"avg_rewards\": avg_rewards\n",
    "    }"
   ],
   "id": "2b10b4400e231269",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot results\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "for lr, result in cartpole_results_lr.items():\n",
    "    plt.plot(result[\"avg_rewards\"], label=lr)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average Reward (3 seeds)\")\n",
    "plt.title(\"DQN Learning Rate Comparison on CartPole\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "60ee8b6260af5228",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "window_size = 20  # Smooth over 20 episodes (adjust as needed)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for lr, result in cartpole_results_lr.items():\n",
    "    rewards = result[\"avg_rewards\"]\n",
    "    # Compute moving average\n",
    "    smoothed_rewards = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "    episodes = np.arange(len(smoothed_rewards))\n",
    "    plt.plot(episodes, smoothed_rewards, label=f\"{lr}\")\n",
    "\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(f\"Smoothed Reward (Window={window_size})\")\n",
    "plt.title(\"DQN Learning Rate Comparison (Moving Average)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "c578473e40fca351",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Hyperparameter Sensitivity (Epsilon Decay)",
   "id": "bdd9d9917d6c99cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "eps_decay_list = [0.99, 0.995, 0.999]\n",
    "cartpole_results_eps_decay = {}\n",
    "\n",
    "for eps_decay in eps_decay_list:\n",
    "    all_rewards = []\n",
    "    for seed in SEEDS:\n",
    "        print(\"eps_decay = \", eps_decay, \"seed = \", seed)\n",
    "        set_seed(seed)\n",
    "        env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "        agent, rewards = DQNAgent.train_dqn(\n",
    "            env=env,\n",
    "            gamma=0.99,\n",
    "            batch_size=32,\n",
    "            epsilon=1.0,\n",
    "            epsilon_decay=eps_decay,\n",
    "            use_target_net=True,\n",
    "            constant_epsilon=False,\n",
    "            num_episodes=2000,\n",
    "            target_update_freq=10,\n",
    "            lr=1e-3,\n",
    "            seed=seed\n",
    "        )\n",
    "\n",
    "        all_rewards.append(rewards)\n",
    "\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    avg_rewards = all_rewards.mean(axis=0)\n",
    "    cartpole_results_eps_decay[f\"eps_decay={eps_decay}\"] = {\n",
    "        \"params\": {\n",
    "            \"lr\": 1e-3,\n",
    "            \"gamma\": 0.99,\n",
    "            \"batch_size\": 32,\n",
    "            \"epsilon_decay\": eps_decay,\n",
    "            \"target_update_freq\": 10\n",
    "        },\n",
    "        \"avg_rewards\": avg_rewards\n",
    "    }"
   ],
   "id": "773eb914cda6861c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot results\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "for eps_decay, result in cartpole_results_eps_decay.items():\n",
    "    plt.plot(result[\"avg_rewards\"], label=eps_decay)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average Reward (3 seeds)\")\n",
    "plt.title(\"DQN Learning Rate Comparison on CartPole\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "93b4381881395a45",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "window_size = 20  # Smooth over 20 episodes (adjust as needed)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for eps_decay, result in cartpole_results_eps_decay.items():\n",
    "    rewards = result[\"avg_rewards\"]\n",
    "    # Compute moving average\n",
    "    smoothed_rewards = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "    episodes = np.arange(len(smoothed_rewards))\n",
    "    plt.plot(episodes, smoothed_rewards, label=f\"{eps_decay}\")\n",
    "\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(f\"Smoothed Reward (Window={window_size})\")\n",
    "plt.title(\"DQN Learning Rate Comparison (Moving Average)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "34abc86b9fc56655",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Hyperparameter Sensitivity (Batch Size)",
   "id": "2c2ae64e4aaaed12"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "batch_sized = [64, 32, 16, 8]\n",
    "cartpole_results_bs = {}\n",
    "\n",
    "for bs in batch_sized:\n",
    "    all_rewards = []\n",
    "    for seed in SEEDS:\n",
    "        print(\"bs = \", bs, \"seed = \", seed)\n",
    "        set_seed(seed)\n",
    "        env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "        agent, rewards = DQNAgent.train_dqn(\n",
    "            env=env,\n",
    "            gamma=0.99,\n",
    "            batch_size=bs,\n",
    "            epsilon=1.0,\n",
    "            epsilon_decay=0.99,\n",
    "            use_target_net=True,\n",
    "            constant_epsilon=False,\n",
    "            num_episodes=2000,\n",
    "            target_update_freq=10,\n",
    "            lr=0.001,\n",
    "            seed=seed\n",
    "        )\n",
    "\n",
    "        all_rewards.append(rewards)\n",
    "\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    avg_rewards = all_rewards.mean(axis=0)\n",
    "    cartpole_results_bs[f\"Batch Size={bs}\"] = {\n",
    "        \"params\": {\n",
    "            \"lr\": 1e-3,\n",
    "            \"gamma\": 0.99,\n",
    "            \"batch_size\": bs,\n",
    "            \"epsilon_decay\": 0.99,\n",
    "            \"target_update_freq\": 10\n",
    "        },\n",
    "        \"avg_rewards\": avg_rewards\n",
    "    }"
   ],
   "id": "1f25440bc3833cff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot results\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "for bs, result in cartpole_results_bs.items():\n",
    "    plt.plot(result[\"avg_rewards\"], label=bs)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average Reward (3 seeds)\")\n",
    "plt.title(\"DQN Learning Rate Comparison on CartPole\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "ff6cf42d6847ec5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "window_size = 20  # Smooth over 20 episodes (adjust as needed)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for batch_size, result in cartpole_results_bs.items():\n",
    "    rewards = result[\"avg_rewards\"]\n",
    "    # Compute moving average\n",
    "    smoothed_rewards = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "    episodes = np.arange(len(smoothed_rewards))\n",
    "    plt.plot(episodes, smoothed_rewards, label=f\"{batch_size}\")\n",
    "\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(f\"Smoothed Reward (Window={window_size})\")\n",
    "plt.title(\"DQN Learning Rate Comparison (Moving Average)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "8e36307b39d0494e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Hyperparameter Sensitivity (Target Net Update Freq)",
   "id": "cb8f5718c55180d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "update_freq = [1, 10, 20, 100]\n",
    "cartpole_results_tf = {}\n",
    "\n",
    "for bs in update_freq:\n",
    "    all_rewards = []\n",
    "    for seed in SEEDS:\n",
    "        print(\"bs = \", bs, \"seed = \", seed)\n",
    "        set_seed(seed)\n",
    "        env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "        agent, rewards = DQNAgent.train_dqn(\n",
    "            env=env,\n",
    "            gamma=0.99,\n",
    "            batch_size=32,\n",
    "            epsilon=1.0,\n",
    "            epsilon_decay=0.99,\n",
    "            use_target_net=True,\n",
    "            constant_epsilon=False,\n",
    "            num_episodes=1500,\n",
    "            target_update_freq=bs,\n",
    "            lr=1e-3,\n",
    "            seed=seed\n",
    "        )\n",
    "\n",
    "        all_rewards.append(rewards)\n",
    "\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    avg_rewards = all_rewards.mean(axis=0)\n",
    "    cartpole_results_tf[f\"Batch Size={bs}\"] = {\n",
    "        \"params\": {\n",
    "            \"lr\": 1e-3,\n",
    "            \"gamma\": 0.99,\n",
    "            \"batch_size\": 32,\n",
    "            \"epsilon_decay\": 0.99,\n",
    "            \"target_update_freq\": bs\n",
    "        },\n",
    "        \"avg_rewards\": avg_rewards\n",
    "    }"
   ],
   "id": "8fe0a4ccc8cbdeb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot results\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "for bs, result in cartpole_results_tf.items():\n",
    "    plt.plot(result[\"avg_rewards\"], label=bs)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average Reward (3 seeds)\")\n",
    "plt.title(\"DQN Learning Rate Comparison on Mountain Car\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "4c790f7d4a78ff48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "window_size = 20  # Smooth over 20 episodes (adjust as needed)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for batch_size, result in cartpole_results_tf.items():\n",
    "    rewards = result[\"avg_rewards\"]\n",
    "    # Compute moving average\n",
    "    smoothed_rewards = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "    episodes = np.arange(len(smoothed_rewards))\n",
    "    plt.plot(episodes, smoothed_rewards, label=f\"{batch_size}\")\n",
    "\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(f\"Smoothed Reward (Window={window_size})\")\n",
    "plt.title(\"DQN Learning Rate Comparison (Moving Average)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "506444e1d5f5de21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Hyperparameter Sensitivity (Network Architecture)",
   "id": "706c5342c133229a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "38b65432400141b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cartpole_rewards1 = []\n",
    "for seed in SEEDS:\n",
    "    set_seed(seed)\n",
    "\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    agent, rewards = DQNAgent.train_dqn(\n",
    "        env=env,\n",
    "        gamma=0.99,\n",
    "        lr=1e-3,\n",
    "        batch_size=64,\n",
    "        epsilon=1.0,\n",
    "        epsilon_decay=0.995,\n",
    "        epsilon_min=0.01,\n",
    "        use_target_net=True,\n",
    "        constant_epsilon=False,\n",
    "        num_episodes=1000,\n",
    "        target_update_freq=10,\n",
    "        seed=42,\n",
    "        smooth_plot=5\n",
    "    )\n",
    "\n",
    "    cartpole_rewards1.append(rewards)"
   ],
   "id": "186e84dd018ed59a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cartpole_rewards1 = np.array(cartpole_rewards1)\n",
    "avg_cartpole_rewards1_per_episode = cartpole_rewards1.mean(axis=0)"
   ],
   "id": "5e334a1ce4cd9724",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a5391e7958f081a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# rolling_avg = np.convolve(avg_cartpole_rewards1_per_episode, np.ones(100)/100, mode='valid')\n",
    "# plt.plot(avg_cartpole_rewards1_per_episode, alpha=0.4, label=\"Reward per Episode\")\n",
    "# plt.plot(rolling_avg, label=\"100-Episode Rolling Average\")\n",
    "# plt.title(\"CartPole-v1: DQN Performance\")\n",
    "# plt.xlabel(\"Episodes\")\n",
    "# plt.ylabel(\"Total Reward\")\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()"
   ],
   "id": "3e44426357af3b9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Ablation Study 1: Effect of QNet Architecture",
   "id": "10d8a5238601cc53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d9ddcbf7b21d8154",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "agent, rewards = DQNAgent.train_dqn(env, constant_epsilon=True)"
   ],
   "id": "50b0d348a771877f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rolling_avg = np.convolve(rewards, np.ones(100)/100, mode='valid')\n",
    "plt.plot(rewards, alpha=0.4, label=\"Reward per Episode\")\n",
    "plt.plot(rolling_avg, label=\"100-Episode Rolling Average\")\n",
    "plt.title(\"CartPole-v1: DQN Performance\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "b6f3fc876a75ee23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Acrobot Environment",
   "id": "c77cba6165fd273e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Hyperparameter Sensitivity (Learning Rate)",
   "id": "e69419dfad1c309f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lrs = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "acrobot_results_lr = {}\n",
    "\n",
    "for lr in lrs:\n",
    "    all_rewards = []\n",
    "    for seed in SEEDS:\n",
    "        print(\"lr = \", lr, \"seed = \", seed)\n",
    "        set_seed(seed)\n",
    "        env = gym.make(\"Acrobot-v1\")\n",
    "\n",
    "        agent, rewards = DQNAgent.train_dqn(\n",
    "            env=env,\n",
    "            gamma=0.99,\n",
    "            batch_size=32,\n",
    "            epsilon=1.0,\n",
    "            epsilon_decay=0.99,\n",
    "            use_target_net=True,\n",
    "            constant_epsilon=False,\n",
    "            num_episodes=1500,\n",
    "            target_update_freq=10,\n",
    "            lr=lr,\n",
    "            seed=seed\n",
    "        )\n",
    "\n",
    "        all_rewards.append(rewards)\n",
    "\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    avg_rewards = all_rewards.mean(axis=0)\n",
    "    acrobot_results_lr[f\"LR={lr}\"] = {\n",
    "        \"params\": {\n",
    "            \"lr\": lr,\n",
    "            \"gamma\": 0.99,\n",
    "            \"batch_size\": 64,\n",
    "            \"epsilon_decay\": 0.99,\n",
    "            \"target_update_freq\": 10\n",
    "        },\n",
    "        \"avg_rewards\": avg_rewards\n",
    "    }"
   ],
   "id": "5946dc81dc54d0fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot results\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "for lr, result in acrobot_results_lr.items():\n",
    "    plt.plot(result[\"avg_rewards\"], label=lr)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average Reward (3 seeds)\")\n",
    "plt.title(\"DQN Learning Rate Comparison on Acrobot\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "1490f850bd11b816",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "window_size = 20  # Smooth over 20 episodes (adjust as needed)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for lr, result in acrobot_results_lr.items():\n",
    "    rewards = result[\"avg_rewards\"]\n",
    "    # Compute moving average\n",
    "    smoothed_rewards = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "    episodes = np.arange(len(smoothed_rewards))\n",
    "    plt.plot(episodes, smoothed_rewards, label=f\"{lr}\")\n",
    "\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(f\"Smoothed Reward (Window={window_size})\")\n",
    "plt.title(\"DQN Learning Rate Comparison (Moving Average)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "c9d2a43a0e133180",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Hyperparameter Sensitivity (Epsilon Decay)",
   "id": "58666a1b77a752f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "eps_decay_list = [0.99, 0.995, 0.999]\n",
    "acrobot_results_eps_decay = {}\n",
    "\n",
    "for eps_decay in eps_decay_list:\n",
    "    all_rewards = []\n",
    "    for seed in SEEDS:\n",
    "        print(\"eps_decay = \", eps_decay, \"seed = \", seed)\n",
    "        set_seed(seed)\n",
    "        env = gym.make(\"Acrobot-v1\")\n",
    "\n",
    "        agent, rewards = DQNAgent.train_dqn(\n",
    "            env=env,\n",
    "            gamma=0.99,\n",
    "            batch_size=32,\n",
    "            epsilon=1.0,\n",
    "            epsilon_decay=eps_decay,\n",
    "            use_target_net=True,\n",
    "            constant_epsilon=False,\n",
    "            num_episodes=1500,\n",
    "            target_update_freq=10,\n",
    "            lr=1e-3,\n",
    "            seed=seed\n",
    "        )\n",
    "\n",
    "        all_rewards.append(rewards)\n",
    "\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    avg_rewards = all_rewards.mean(axis=0)\n",
    "    acrobot_results_eps_decay[f\"eps_decay={eps_decay}\"] = {\n",
    "        \"params\": {\n",
    "            \"lr\": 1e-3,\n",
    "            \"gamma\": 0.99,\n",
    "            \"batch_size\": 32,\n",
    "            \"epsilon_decay\": eps_decay,\n",
    "            \"target_update_freq\": 10\n",
    "        },\n",
    "        \"avg_rewards\": avg_rewards\n",
    "    }"
   ],
   "id": "b7262011bbe34cdb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot results\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "for eps_decay, result in acrobot_results_eps_decay.items():\n",
    "    plt.plot(result[\"avg_rewards\"], label=eps_decay)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average Reward (3 seeds)\")\n",
    "plt.title(\"DQN Learning Rate Comparison on Acrobot\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "cfc69494a8523581",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "window_size = 10  # Smooth over 20 episodes (adjust as needed)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for eps_decay, result in acrobot_results_eps_decay.items():\n",
    "    rewards = result[\"avg_rewards\"]\n",
    "    # Compute moving average\n",
    "    smoothed_rewards = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "    episodes = np.arange(len(smoothed_rewards))\n",
    "    plt.plot(episodes, smoothed_rewards, label=f\"{eps_decay}\")\n",
    "\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(f\"Smoothed Reward (Window={window_size})\")\n",
    "plt.title(\"DQN Learning Rate Comparison (Moving Average)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "34b20abfec192d40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Hyperparameter Sensitivity (Batch Size)",
   "id": "b65b5f97b20f59ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "batch_sized = [64, 32, 16]\n",
    "acrobot_results_bs = {}\n",
    "\n",
    "for bs in batch_sized:\n",
    "    all_rewards = []\n",
    "    for seed in SEEDS:\n",
    "        print(\"bs = \", bs, \"seed = \", seed)\n",
    "        set_seed(seed)\n",
    "        env = gym.make(\"Acrobot-v1\")\n",
    "\n",
    "        agent, rewards = DQNAgent.train_dqn(\n",
    "            env=env,\n",
    "            gamma=0.99,\n",
    "            batch_size=bs,\n",
    "            epsilon=1.0,\n",
    "            epsilon_decay=0.99,\n",
    "            use_target_net=True,\n",
    "            constant_epsilon=False,\n",
    "            num_episodes=1500,\n",
    "            target_update_freq=10,\n",
    "            lr=1e-3,\n",
    "            seed=seed\n",
    "        )\n",
    "\n",
    "        all_rewards.append(rewards)\n",
    "\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    avg_rewards = all_rewards.mean(axis=0)\n",
    "    acrobot_results_bs[f\"Batch Size={bs}\"] = {\n",
    "        \"params\": {\n",
    "            \"lr\": 1e-3,\n",
    "            \"gamma\": 0.99,\n",
    "            \"batch_size\": bs,\n",
    "            \"epsilon_decay\": 0.99,\n",
    "            \"target_update_freq\": 10\n",
    "        },\n",
    "        \"avg_rewards\": avg_rewards\n",
    "    }"
   ],
   "id": "2dd6cfcc4dc9b157",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot results\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "for bs, result in acrobot_results_bs.items():\n",
    "    plt.plot(result[\"avg_rewards\"], label=bs)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average Reward (3 seeds)\")\n",
    "plt.title(\"DQN Learning Rate Comparison on Acrobot\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "d80b5d503ed4c7d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "window_size = 20  # Smooth over 20 episodes (adjust as needed)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for batch_size, result in acrobot_results_bs.items():\n",
    "    rewards = result[\"avg_rewards\"]\n",
    "    # Compute moving average\n",
    "    smoothed_rewards = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "    episodes = np.arange(len(smoothed_rewards))\n",
    "    plt.plot(episodes, smoothed_rewards, label=f\"{batch_size}\")\n",
    "\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(f\"Smoothed Reward (Window={window_size})\")\n",
    "plt.title(\"DQN Learning Rate Comparison (Moving Average)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "dbda4589cf4b7b9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Hyperparameter Sensitivity (Target Net Update Freq)",
   "id": "eb8b1f457246a76f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "update_freq = [1, 10, 20, 100]\n",
    "acrobot_results_tf = {}\n",
    "\n",
    "for bs in update_freq:\n",
    "    all_rewards = []\n",
    "    for seed in SEEDS:\n",
    "        print(\"bs = \", bs, \"seed = \", seed)\n",
    "        set_seed(seed)\n",
    "        env = gym.make(\"Acrobot-v1\")\n",
    "\n",
    "        agent, rewards = DQNAgent.train_dqn(\n",
    "            env=env,\n",
    "            gamma=0.99,\n",
    "            batch_size=32,\n",
    "            epsilon=1.0,\n",
    "            epsilon_decay=0.99,\n",
    "            use_target_net=True,\n",
    "            constant_epsilon=False,\n",
    "            num_episodes=1500,\n",
    "            target_update_freq=bs,\n",
    "            lr=1e-3,\n",
    "            seed=seed\n",
    "        )\n",
    "\n",
    "        all_rewards.append(rewards)\n",
    "\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    avg_rewards = all_rewards.mean(axis=0)\n",
    "    acrobot_results_tf[f\"Batch Size={bs}\"] = {\n",
    "        \"params\": {\n",
    "            \"lr\": 1e-3,\n",
    "            \"gamma\": 0.99,\n",
    "            \"batch_size\": 32,\n",
    "            \"epsilon_decay\": 0.99,\n",
    "            \"target_update_freq\": bs\n",
    "        },\n",
    "        \"avg_rewards\": avg_rewards\n",
    "    }"
   ],
   "id": "938bc49695e64fe0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot results\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "for bs, result in acrobot_results_tf.items():\n",
    "    plt.plot(result[\"avg_rewards\"], label=bs)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average Reward (3 seeds)\")\n",
    "plt.title(\"DQN Learning Rate Comparison on Mountain Car\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "76d38a896b02d190",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "window_size = 20  # Smooth over 20 episodes (adjust as needed)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for batch_size, result in acrobot_results_tf.items():\n",
    "    rewards = result[\"avg_rewards\"]\n",
    "    # Compute moving average\n",
    "    smoothed_rewards = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "    episodes = np.arange(len(smoothed_rewards))\n",
    "    plt.plot(episodes, smoothed_rewards, label=f\"{batch_size}\")\n",
    "\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(f\"Smoothed Reward (Window={window_size})\")\n",
    "plt.title(\"DQN Learning Rate Comparison (Moving Average)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "508229b557877a9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Ablation Study 1: Effect of QNet",
   "id": "ce621c4a8f7f696"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e7f987446205404a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Ablation Study 2: Constant Epsilon",
   "id": "728516896a1dc960"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "70fae540dbf72e33",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MountainCar Environment",
   "id": "ee387757c6db310e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T10:48:56.427041Z",
     "start_time": "2025-05-19T10:48:56.413726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "dqn_agent = DQNAgent(\n",
    "    env=env,\n",
    "    gamma=0.99,\n",
    "    lr=1e-3,\n",
    "    batch_size=128,\n",
    "    epsilon=1.0,\n",
    "    epsilon_decay=0.995,\n",
    "    epsilon_min=0.01,\n",
    "    hidden_dim=128,\n",
    "    tau=1.0,\n",
    "    num_episodes=500,\n",
    "    eval_interval=5,\n",
    "    target_update_freq=10,\n",
    "    constant_epsilon=False\n",
    ")"
   ],
   "id": "fdb3690fe3de0088",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T10:49:46.866175Z",
     "start_time": "2025-05-19T10:48:57.761093Z"
    }
   },
   "cell_type": "code",
   "source": "rewards_history, eval_rewards_history = dqn_agent.train_with_seed()",
   "id": "45c17c2b05167dd0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total reward: -200.0, Epsilon: 0.995\n",
      "Episode 1, Total reward: -200.0, Epsilon: 0.990\n",
      "Episode 2, Total reward: -200.0, Epsilon: 0.985\n",
      "Episode 3, Total reward: -200.0, Epsilon: 0.980\n",
      "Episode 4, Total reward: -200.0, Epsilon: 0.975\n",
      "Episode 5, Total reward: -200.0, Epsilon: 0.970\n",
      "Episode 6, Total reward: -200.0, Epsilon: 0.966\n",
      "Episode 7, Total reward: -200.0, Epsilon: 0.961\n",
      "Episode 8, Total reward: -200.0, Epsilon: 0.956\n",
      "Episode 9, Total reward: -200.0, Epsilon: 0.951\n",
      "Episode 10, Total reward: -200.0, Epsilon: 0.946\n",
      "Episode 11, Total reward: -200.0, Epsilon: 0.942\n",
      "Episode 12, Total reward: -200.0, Epsilon: 0.937\n",
      "Episode 13, Total reward: -200.0, Epsilon: 0.932\n",
      "Episode 14, Total reward: -200.0, Epsilon: 0.928\n",
      "Episode 15, Total reward: -200.0, Epsilon: 0.923\n",
      "Episode 16, Total reward: -200.0, Epsilon: 0.918\n",
      "Episode 17, Total reward: -200.0, Epsilon: 0.914\n",
      "Episode 18, Total reward: -200.0, Epsilon: 0.909\n",
      "Episode 19, Total reward: -200.0, Epsilon: 0.905\n",
      "Episode 20, Total reward: -200.0, Epsilon: 0.900\n",
      "Episode 21, Total reward: -200.0, Epsilon: 0.896\n",
      "Episode 22, Total reward: -200.0, Epsilon: 0.891\n",
      "Episode 23, Total reward: -200.0, Epsilon: 0.887\n",
      "Episode 24, Total reward: -200.0, Epsilon: 0.882\n",
      "Episode 25, Total reward: -200.0, Epsilon: 0.878\n",
      "Episode 26, Total reward: -200.0, Epsilon: 0.873\n",
      "Episode 27, Total reward: -200.0, Epsilon: 0.869\n",
      "Episode 28, Total reward: -200.0, Epsilon: 0.865\n",
      "Episode 29, Total reward: -200.0, Epsilon: 0.860\n",
      "Episode 30, Total reward: -200.0, Epsilon: 0.856\n",
      "Episode 31, Total reward: -200.0, Epsilon: 0.852\n",
      "Episode 32, Total reward: -200.0, Epsilon: 0.848\n",
      "Episode 33, Total reward: -200.0, Epsilon: 0.843\n",
      "Episode 34, Total reward: -200.0, Epsilon: 0.839\n",
      "Episode 35, Total reward: -200.0, Epsilon: 0.835\n",
      "Episode 36, Total reward: -200.0, Epsilon: 0.831\n",
      "Episode 37, Total reward: -200.0, Epsilon: 0.827\n",
      "Episode 38, Total reward: -200.0, Epsilon: 0.822\n",
      "Episode 39, Total reward: -200.0, Epsilon: 0.818\n",
      "Episode 40, Total reward: -200.0, Epsilon: 0.814\n",
      "Episode 41, Total reward: -200.0, Epsilon: 0.810\n",
      "Episode 42, Total reward: -200.0, Epsilon: 0.806\n",
      "Episode 43, Total reward: -200.0, Epsilon: 0.802\n",
      "Episode 44, Total reward: -200.0, Epsilon: 0.798\n",
      "Episode 45, Total reward: -200.0, Epsilon: 0.794\n",
      "Episode 46, Total reward: -200.0, Epsilon: 0.790\n",
      "Episode 47, Total reward: -200.0, Epsilon: 0.786\n",
      "Episode 48, Total reward: -200.0, Epsilon: 0.782\n",
      "Episode 49, Total reward: -200.0, Epsilon: 0.778\n",
      "Episode 50, Total reward: -200.0, Epsilon: 0.774\n",
      "Episode 51, Total reward: -200.0, Epsilon: 0.771\n",
      "Episode 52, Total reward: -200.0, Epsilon: 0.767\n",
      "Episode 53, Total reward: -200.0, Epsilon: 0.763\n",
      "Episode 54, Total reward: -200.0, Epsilon: 0.759\n",
      "Episode 55, Total reward: -200.0, Epsilon: 0.755\n",
      "Episode 56, Total reward: -200.0, Epsilon: 0.751\n",
      "Episode 57, Total reward: -200.0, Epsilon: 0.748\n",
      "Episode 58, Total reward: -200.0, Epsilon: 0.744\n",
      "Episode 59, Total reward: -200.0, Epsilon: 0.740\n",
      "Episode 60, Total reward: -200.0, Epsilon: 0.737\n",
      "Episode 61, Total reward: -200.0, Epsilon: 0.733\n",
      "Episode 62, Total reward: -200.0, Epsilon: 0.729\n",
      "Episode 63, Total reward: -200.0, Epsilon: 0.726\n",
      "Episode 64, Total reward: -200.0, Epsilon: 0.722\n",
      "Episode 65, Total reward: -200.0, Epsilon: 0.718\n",
      "Episode 66, Total reward: -200.0, Epsilon: 0.715\n",
      "Episode 67, Total reward: -200.0, Epsilon: 0.711\n",
      "Episode 68, Total reward: -200.0, Epsilon: 0.708\n",
      "Episode 69, Total reward: -200.0, Epsilon: 0.704\n",
      "Episode 70, Total reward: -200.0, Epsilon: 0.701\n",
      "Episode 71, Total reward: -200.0, Epsilon: 0.697\n",
      "Episode 72, Total reward: -200.0, Epsilon: 0.694\n",
      "Episode 73, Total reward: -200.0, Epsilon: 0.690\n",
      "Episode 74, Total reward: -200.0, Epsilon: 0.687\n",
      "Episode 75, Total reward: -200.0, Epsilon: 0.683\n",
      "Episode 76, Total reward: -200.0, Epsilon: 0.680\n",
      "Episode 77, Total reward: -200.0, Epsilon: 0.676\n",
      "Episode 78, Total reward: -200.0, Epsilon: 0.673\n",
      "Episode 79, Total reward: -200.0, Epsilon: 0.670\n",
      "Episode 80, Total reward: -200.0, Epsilon: 0.666\n",
      "Episode 81, Total reward: -200.0, Epsilon: 0.663\n",
      "Episode 82, Total reward: -200.0, Epsilon: 0.660\n",
      "Episode 83, Total reward: -200.0, Epsilon: 0.656\n",
      "Episode 84, Total reward: -200.0, Epsilon: 0.653\n",
      "Episode 85, Total reward: -200.0, Epsilon: 0.650\n",
      "Episode 86, Total reward: -200.0, Epsilon: 0.647\n",
      "Episode 87, Total reward: -200.0, Epsilon: 0.643\n",
      "Episode 88, Total reward: -200.0, Epsilon: 0.640\n",
      "Episode 89, Total reward: -200.0, Epsilon: 0.637\n",
      "Episode 90, Total reward: -200.0, Epsilon: 0.634\n",
      "Episode 91, Total reward: -200.0, Epsilon: 0.631\n",
      "Episode 92, Total reward: -200.0, Epsilon: 0.627\n",
      "Episode 93, Total reward: -200.0, Epsilon: 0.624\n",
      "Episode 94, Total reward: -200.0, Epsilon: 0.621\n",
      "Episode 95, Total reward: -200.0, Epsilon: 0.618\n",
      "Episode 96, Total reward: -200.0, Epsilon: 0.615\n",
      "Episode 97, Total reward: -200.0, Epsilon: 0.612\n",
      "Episode 98, Total reward: -200.0, Epsilon: 0.609\n",
      "Episode 99, Total reward: -200.0, Epsilon: 0.606\n",
      "Episode 100, Total reward: -200.0, Epsilon: 0.603\n",
      "Episode 101, Total reward: -200.0, Epsilon: 0.600\n",
      "Episode 102, Total reward: -200.0, Epsilon: 0.597\n",
      "Episode 103, Total reward: -200.0, Epsilon: 0.594\n",
      "Episode 104, Total reward: -200.0, Epsilon: 0.591\n",
      "Episode 105, Total reward: -200.0, Epsilon: 0.588\n",
      "Episode 106, Total reward: -200.0, Epsilon: 0.585\n",
      "Episode 107, Total reward: -200.0, Epsilon: 0.582\n",
      "Episode 108, Total reward: -200.0, Epsilon: 0.579\n",
      "Episode 109, Total reward: -200.0, Epsilon: 0.576\n",
      "Episode 110, Total reward: -200.0, Epsilon: 0.573\n",
      "Episode 111, Total reward: -200.0, Epsilon: 0.570\n",
      "Episode 112, Total reward: -200.0, Epsilon: 0.568\n",
      "Episode 113, Total reward: -200.0, Epsilon: 0.565\n",
      "Episode 114, Total reward: -200.0, Epsilon: 0.562\n",
      "Episode 115, Total reward: -200.0, Epsilon: 0.559\n",
      "Episode 116, Total reward: -200.0, Epsilon: 0.556\n",
      "Episode 117, Total reward: -200.0, Epsilon: 0.554\n",
      "Episode 118, Total reward: -200.0, Epsilon: 0.551\n",
      "Episode 119, Total reward: -200.0, Epsilon: 0.548\n",
      "Episode 120, Total reward: -200.0, Epsilon: 0.545\n",
      "Episode 121, Total reward: -200.0, Epsilon: 0.543\n",
      "Episode 122, Total reward: -200.0, Epsilon: 0.540\n",
      "Episode 123, Total reward: -200.0, Epsilon: 0.537\n",
      "Episode 124, Total reward: -200.0, Epsilon: 0.534\n",
      "Episode 125, Total reward: -200.0, Epsilon: 0.532\n",
      "Episode 126, Total reward: -200.0, Epsilon: 0.529\n",
      "Episode 127, Total reward: -200.0, Epsilon: 0.526\n",
      "Episode 128, Total reward: -200.0, Epsilon: 0.524\n",
      "Episode 129, Total reward: -200.0, Epsilon: 0.521\n",
      "Episode 130, Total reward: -200.0, Epsilon: 0.519\n",
      "Episode 131, Total reward: -200.0, Epsilon: 0.516\n",
      "Episode 132, Total reward: -200.0, Epsilon: 0.513\n",
      "Episode 133, Total reward: -200.0, Epsilon: 0.511\n",
      "Episode 134, Total reward: -200.0, Epsilon: 0.508\n",
      "Episode 135, Total reward: -200.0, Epsilon: 0.506\n",
      "Episode 136, Total reward: -200.0, Epsilon: 0.503\n",
      "Episode 137, Total reward: -200.0, Epsilon: 0.501\n",
      "Episode 138, Total reward: -200.0, Epsilon: 0.498\n",
      "Episode 139, Total reward: -200.0, Epsilon: 0.496\n",
      "Episode 140, Total reward: -200.0, Epsilon: 0.493\n",
      "Episode 141, Total reward: -200.0, Epsilon: 0.491\n",
      "Episode 142, Total reward: -200.0, Epsilon: 0.488\n",
      "Episode 143, Total reward: -200.0, Epsilon: 0.486\n",
      "Episode 144, Total reward: -200.0, Epsilon: 0.483\n",
      "Episode 145, Total reward: -200.0, Epsilon: 0.481\n",
      "Episode 146, Total reward: -200.0, Epsilon: 0.479\n",
      "Episode 147, Total reward: -200.0, Epsilon: 0.476\n",
      "Episode 148, Total reward: -200.0, Epsilon: 0.474\n",
      "Episode 149, Total reward: -200.0, Epsilon: 0.471\n",
      "Episode 150, Total reward: -200.0, Epsilon: 0.469\n",
      "Episode 151, Total reward: -200.0, Epsilon: 0.467\n",
      "Episode 152, Total reward: -200.0, Epsilon: 0.464\n",
      "Episode 153, Total reward: -200.0, Epsilon: 0.462\n",
      "Episode 154, Total reward: -200.0, Epsilon: 0.460\n",
      "Episode 155, Total reward: -200.0, Epsilon: 0.458\n",
      "Episode 156, Total reward: -200.0, Epsilon: 0.455\n",
      "Episode 157, Total reward: -200.0, Epsilon: 0.453\n",
      "Episode 158, Total reward: -200.0, Epsilon: 0.451\n",
      "Episode 159, Total reward: -200.0, Epsilon: 0.448\n",
      "Episode 160, Total reward: -200.0, Epsilon: 0.446\n",
      "Episode 161, Total reward: -200.0, Epsilon: 0.444\n",
      "Episode 162, Total reward: -200.0, Epsilon: 0.442\n",
      "Episode 163, Total reward: -200.0, Epsilon: 0.440\n",
      "Episode 164, Total reward: -200.0, Epsilon: 0.437\n",
      "Episode 165, Total reward: -200.0, Epsilon: 0.435\n",
      "Episode 166, Total reward: -200.0, Epsilon: 0.433\n",
      "Episode 167, Total reward: -200.0, Epsilon: 0.431\n",
      "Episode 168, Total reward: -200.0, Epsilon: 0.429\n",
      "Episode 169, Total reward: -200.0, Epsilon: 0.427\n",
      "Episode 170, Total reward: -200.0, Epsilon: 0.424\n",
      "Episode 171, Total reward: -200.0, Epsilon: 0.422\n",
      "Episode 172, Total reward: -200.0, Epsilon: 0.420\n",
      "Episode 173, Total reward: -200.0, Epsilon: 0.418\n",
      "Episode 174, Total reward: -200.0, Epsilon: 0.416\n",
      "Episode 175, Total reward: -200.0, Epsilon: 0.414\n",
      "Episode 176, Total reward: -200.0, Epsilon: 0.412\n",
      "Episode 177, Total reward: -200.0, Epsilon: 0.410\n",
      "Episode 178, Total reward: -200.0, Epsilon: 0.408\n",
      "Episode 179, Total reward: -200.0, Epsilon: 0.406\n",
      "Episode 180, Total reward: -200.0, Epsilon: 0.404\n",
      "Episode 181, Total reward: -200.0, Epsilon: 0.402\n",
      "Episode 182, Total reward: -200.0, Epsilon: 0.400\n",
      "Episode 183, Total reward: -200.0, Epsilon: 0.398\n",
      "Episode 184, Total reward: -200.0, Epsilon: 0.396\n",
      "Episode 185, Total reward: -200.0, Epsilon: 0.394\n",
      "Episode 186, Total reward: -200.0, Epsilon: 0.392\n",
      "Episode 187, Total reward: -200.0, Epsilon: 0.390\n",
      "Episode 188, Total reward: -200.0, Epsilon: 0.388\n",
      "Episode 189, Total reward: -200.0, Epsilon: 0.386\n",
      "Episode 190, Total reward: -200.0, Epsilon: 0.384\n",
      "Episode 191, Total reward: -200.0, Epsilon: 0.382\n",
      "Episode 192, Total reward: -200.0, Epsilon: 0.380\n",
      "Episode 193, Total reward: -200.0, Epsilon: 0.378\n",
      "Episode 194, Total reward: -200.0, Epsilon: 0.376\n",
      "Episode 195, Total reward: -200.0, Epsilon: 0.374\n",
      "Episode 196, Total reward: -200.0, Epsilon: 0.373\n",
      "Episode 197, Total reward: -200.0, Epsilon: 0.371\n",
      "Episode 198, Total reward: -200.0, Epsilon: 0.369\n",
      "Episode 199, Total reward: -200.0, Epsilon: 0.367\n",
      "Episode 200, Total reward: -200.0, Epsilon: 0.365\n",
      "Episode 201, Total reward: -200.0, Epsilon: 0.363\n",
      "Episode 202, Total reward: -200.0, Epsilon: 0.361\n",
      "Episode 203, Total reward: -200.0, Epsilon: 0.360\n",
      "Episode 204, Total reward: -200.0, Epsilon: 0.358\n",
      "Episode 205, Total reward: -200.0, Epsilon: 0.356\n",
      "Episode 206, Total reward: -200.0, Epsilon: 0.354\n",
      "Episode 207, Total reward: -200.0, Epsilon: 0.353\n",
      "Episode 208, Total reward: -200.0, Epsilon: 0.351\n",
      "Episode 209, Total reward: -200.0, Epsilon: 0.349\n",
      "Episode 210, Total reward: -200.0, Epsilon: 0.347\n",
      "Episode 211, Total reward: -200.0, Epsilon: 0.346\n",
      "Episode 212, Total reward: -200.0, Epsilon: 0.344\n",
      "Episode 213, Total reward: -200.0, Epsilon: 0.342\n",
      "Episode 214, Total reward: -200.0, Epsilon: 0.340\n",
      "Episode 215, Total reward: -200.0, Epsilon: 0.339\n",
      "Episode 216, Total reward: -200.0, Epsilon: 0.337\n",
      "Episode 217, Total reward: -200.0, Epsilon: 0.335\n",
      "Episode 218, Total reward: -200.0, Epsilon: 0.334\n",
      "Episode 219, Total reward: -200.0, Epsilon: 0.332\n",
      "Episode 220, Total reward: -200.0, Epsilon: 0.330\n",
      "Episode 221, Total reward: -200.0, Epsilon: 0.329\n",
      "Episode 222, Total reward: -200.0, Epsilon: 0.327\n",
      "Episode 223, Total reward: -200.0, Epsilon: 0.325\n",
      "Episode 224, Total reward: -200.0, Epsilon: 0.324\n",
      "Episode 225, Total reward: -200.0, Epsilon: 0.322\n",
      "Episode 226, Total reward: -200.0, Epsilon: 0.321\n",
      "Episode 227, Total reward: -200.0, Epsilon: 0.319\n",
      "Episode 228, Total reward: -200.0, Epsilon: 0.317\n",
      "Episode 229, Total reward: -200.0, Epsilon: 0.316\n",
      "Episode 230, Total reward: -200.0, Epsilon: 0.314\n",
      "Episode 231, Total reward: -200.0, Epsilon: 0.313\n",
      "Episode 232, Total reward: -200.0, Epsilon: 0.311\n",
      "Episode 233, Total reward: -200.0, Epsilon: 0.309\n",
      "Episode 234, Total reward: -200.0, Epsilon: 0.308\n",
      "Episode 235, Total reward: -200.0, Epsilon: 0.306\n",
      "Episode 236, Total reward: -200.0, Epsilon: 0.305\n",
      "Episode 237, Total reward: -200.0, Epsilon: 0.303\n",
      "Episode 238, Total reward: -200.0, Epsilon: 0.302\n",
      "Episode 239, Total reward: -200.0, Epsilon: 0.300\n",
      "Episode 240, Total reward: -200.0, Epsilon: 0.299\n",
      "Episode 241, Total reward: -200.0, Epsilon: 0.297\n",
      "Episode 242, Total reward: -200.0, Epsilon: 0.296\n",
      "Episode 243, Total reward: -200.0, Epsilon: 0.294\n",
      "Episode 244, Total reward: -200.0, Epsilon: 0.293\n",
      "Episode 245, Total reward: -200.0, Epsilon: 0.291\n",
      "Episode 246, Total reward: -200.0, Epsilon: 0.290\n",
      "Episode 247, Total reward: -200.0, Epsilon: 0.288\n",
      "Episode 248, Total reward: -200.0, Epsilon: 0.287\n",
      "Episode 249, Total reward: -200.0, Epsilon: 0.286\n",
      "Episode 250, Total reward: -200.0, Epsilon: 0.284\n",
      "Episode 251, Total reward: -200.0, Epsilon: 0.283\n",
      "Episode 252, Total reward: -200.0, Epsilon: 0.281\n",
      "Episode 253, Total reward: -200.0, Epsilon: 0.280\n",
      "Episode 254, Total reward: -200.0, Epsilon: 0.279\n",
      "Episode 255, Total reward: -200.0, Epsilon: 0.277\n",
      "Episode 256, Total reward: -200.0, Epsilon: 0.276\n",
      "Episode 257, Total reward: -200.0, Epsilon: 0.274\n",
      "Episode 258, Total reward: -200.0, Epsilon: 0.273\n",
      "Episode 259, Total reward: -200.0, Epsilon: 0.272\n",
      "Episode 260, Total reward: -200.0, Epsilon: 0.270\n",
      "Episode 261, Total reward: -200.0, Epsilon: 0.269\n",
      "Episode 262, Total reward: -200.0, Epsilon: 0.268\n",
      "Episode 263, Total reward: -200.0, Epsilon: 0.266\n",
      "Episode 264, Total reward: -182.0, Epsilon: 0.265\n",
      "Episode 265, Total reward: -200.0, Epsilon: 0.264\n",
      "Episode 266, Total reward: -200.0, Epsilon: 0.262\n",
      "Episode 267, Total reward: -200.0, Epsilon: 0.261\n",
      "Episode 268, Total reward: -200.0, Epsilon: 0.260\n",
      "Episode 269, Total reward: -200.0, Epsilon: 0.258\n",
      "Episode 270, Total reward: -200.0, Epsilon: 0.257\n",
      "Episode 271, Total reward: -200.0, Epsilon: 0.256\n",
      "Episode 272, Total reward: -200.0, Epsilon: 0.255\n",
      "Episode 273, Total reward: -200.0, Epsilon: 0.253\n",
      "Episode 274, Total reward: -191.0, Epsilon: 0.252\n",
      "Episode 275, Total reward: -200.0, Epsilon: 0.251\n",
      "Episode 276, Total reward: -200.0, Epsilon: 0.249\n",
      "Episode 277, Total reward: -200.0, Epsilon: 0.248\n",
      "Episode 278, Total reward: -200.0, Epsilon: 0.247\n",
      "Episode 279, Total reward: -200.0, Epsilon: 0.246\n",
      "Episode 280, Total reward: -200.0, Epsilon: 0.245\n",
      "Episode 281, Total reward: -200.0, Epsilon: 0.243\n",
      "Episode 282, Total reward: -200.0, Epsilon: 0.242\n",
      "Episode 283, Total reward: -200.0, Epsilon: 0.241\n",
      "Episode 284, Total reward: -200.0, Epsilon: 0.240\n",
      "Episode 285, Total reward: -200.0, Epsilon: 0.238\n",
      "Episode 286, Total reward: -200.0, Epsilon: 0.237\n",
      "Episode 287, Total reward: -200.0, Epsilon: 0.236\n",
      "Episode 288, Total reward: -200.0, Epsilon: 0.235\n",
      "Episode 289, Total reward: -166.0, Epsilon: 0.234\n",
      "Episode 290, Total reward: -200.0, Epsilon: 0.233\n",
      "Episode 291, Total reward: -186.0, Epsilon: 0.231\n",
      "Episode 292, Total reward: -168.0, Epsilon: 0.230\n",
      "Episode 293, Total reward: -200.0, Epsilon: 0.229\n",
      "Episode 294, Total reward: -180.0, Epsilon: 0.228\n",
      "Episode 295, Total reward: -200.0, Epsilon: 0.227\n",
      "Episode 296, Total reward: -200.0, Epsilon: 0.226\n",
      "Episode 297, Total reward: -200.0, Epsilon: 0.225\n",
      "Episode 298, Total reward: -200.0, Epsilon: 0.223\n",
      "Episode 299, Total reward: -200.0, Epsilon: 0.222\n",
      "Episode 300, Total reward: -200.0, Epsilon: 0.221\n",
      "Episode 301, Total reward: -200.0, Epsilon: 0.220\n",
      "Episode 302, Total reward: -200.0, Epsilon: 0.219\n",
      "Episode 303, Total reward: -200.0, Epsilon: 0.218\n",
      "Episode 304, Total reward: -200.0, Epsilon: 0.217\n",
      "Episode 305, Total reward: -200.0, Epsilon: 0.216\n",
      "Episode 306, Total reward: -200.0, Epsilon: 0.215\n",
      "Episode 307, Total reward: -200.0, Epsilon: 0.214\n",
      "Episode 308, Total reward: -200.0, Epsilon: 0.212\n",
      "Episode 309, Total reward: -187.0, Epsilon: 0.211\n",
      "Episode 310, Total reward: -200.0, Epsilon: 0.210\n",
      "Episode 311, Total reward: -196.0, Epsilon: 0.209\n",
      "Episode 312, Total reward: -159.0, Epsilon: 0.208\n",
      "Episode 313, Total reward: -172.0, Epsilon: 0.207\n",
      "Episode 314, Total reward: -200.0, Epsilon: 0.206\n",
      "Episode 315, Total reward: -200.0, Epsilon: 0.205\n",
      "Episode 316, Total reward: -200.0, Epsilon: 0.204\n",
      "Episode 317, Total reward: -200.0, Epsilon: 0.203\n",
      "Episode 318, Total reward: -200.0, Epsilon: 0.202\n",
      "Episode 319, Total reward: -200.0, Epsilon: 0.201\n",
      "Episode 320, Total reward: -200.0, Epsilon: 0.200\n",
      "Episode 321, Total reward: -200.0, Epsilon: 0.199\n",
      "Episode 322, Total reward: -200.0, Epsilon: 0.198\n",
      "Episode 323, Total reward: -200.0, Epsilon: 0.197\n",
      "Episode 324, Total reward: -192.0, Epsilon: 0.196\n",
      "Episode 325, Total reward: -200.0, Epsilon: 0.195\n",
      "Episode 326, Total reward: -200.0, Epsilon: 0.194\n",
      "Episode 327, Total reward: -200.0, Epsilon: 0.193\n",
      "Episode 328, Total reward: -200.0, Epsilon: 0.192\n",
      "Episode 329, Total reward: -200.0, Epsilon: 0.191\n",
      "Episode 330, Total reward: -200.0, Epsilon: 0.190\n",
      "Episode 331, Total reward: -200.0, Epsilon: 0.189\n",
      "Episode 332, Total reward: -184.0, Epsilon: 0.188\n",
      "Episode 333, Total reward: -200.0, Epsilon: 0.187\n",
      "Episode 334, Total reward: -200.0, Epsilon: 0.187\n",
      "Episode 335, Total reward: -178.0, Epsilon: 0.186\n",
      "Episode 336, Total reward: -186.0, Epsilon: 0.185\n",
      "Episode 337, Total reward: -200.0, Epsilon: 0.184\n",
      "Episode 338, Total reward: -200.0, Epsilon: 0.183\n",
      "Episode 339, Total reward: -157.0, Epsilon: 0.182\n",
      "Episode 340, Total reward: -187.0, Epsilon: 0.181\n",
      "Episode 341, Total reward: -200.0, Epsilon: 0.180\n",
      "Episode 342, Total reward: -176.0, Epsilon: 0.179\n",
      "Episode 343, Total reward: -165.0, Epsilon: 0.178\n",
      "Episode 344, Total reward: -138.0, Epsilon: 0.177\n",
      "Episode 345, Total reward: -200.0, Epsilon: 0.177\n",
      "Episode 346, Total reward: -154.0, Epsilon: 0.176\n",
      "Episode 347, Total reward: -167.0, Epsilon: 0.175\n",
      "Episode 348, Total reward: -187.0, Epsilon: 0.174\n",
      "Episode 349, Total reward: -200.0, Epsilon: 0.173\n",
      "Episode 350, Total reward: -176.0, Epsilon: 0.172\n",
      "Episode 351, Total reward: -200.0, Epsilon: 0.171\n",
      "Episode 352, Total reward: -200.0, Epsilon: 0.170\n",
      "Episode 353, Total reward: -166.0, Epsilon: 0.170\n",
      "Episode 354, Total reward: -181.0, Epsilon: 0.169\n",
      "Episode 355, Total reward: -129.0, Epsilon: 0.168\n",
      "Episode 356, Total reward: -189.0, Epsilon: 0.167\n",
      "Episode 357, Total reward: -197.0, Epsilon: 0.166\n",
      "Episode 358, Total reward: -172.0, Epsilon: 0.165\n",
      "Episode 359, Total reward: -185.0, Epsilon: 0.165\n",
      "Episode 360, Total reward: -200.0, Epsilon: 0.164\n",
      "Episode 361, Total reward: -196.0, Epsilon: 0.163\n",
      "Episode 362, Total reward: -164.0, Epsilon: 0.162\n",
      "Episode 363, Total reward: -128.0, Epsilon: 0.161\n",
      "Episode 364, Total reward: -135.0, Epsilon: 0.160\n",
      "Episode 365, Total reward: -193.0, Epsilon: 0.160\n",
      "Episode 366, Total reward: -163.0, Epsilon: 0.159\n",
      "Episode 367, Total reward: -200.0, Epsilon: 0.158\n",
      "Episode 368, Total reward: -175.0, Epsilon: 0.157\n",
      "Episode 369, Total reward: -200.0, Epsilon: 0.157\n",
      "Episode 370, Total reward: -200.0, Epsilon: 0.156\n",
      "Episode 371, Total reward: -127.0, Epsilon: 0.155\n",
      "Episode 372, Total reward: -200.0, Epsilon: 0.154\n",
      "Episode 373, Total reward: -126.0, Epsilon: 0.153\n",
      "Episode 374, Total reward: -141.0, Epsilon: 0.153\n",
      "Episode 375, Total reward: -165.0, Epsilon: 0.152\n",
      "Episode 376, Total reward: -154.0, Epsilon: 0.151\n",
      "Episode 377, Total reward: -117.0, Epsilon: 0.150\n",
      "Episode 378, Total reward: -163.0, Epsilon: 0.150\n",
      "Episode 379, Total reward: -200.0, Epsilon: 0.149\n",
      "Episode 380, Total reward: -158.0, Epsilon: 0.148\n",
      "Episode 381, Total reward: -176.0, Epsilon: 0.147\n",
      "Episode 382, Total reward: -178.0, Epsilon: 0.147\n",
      "Episode 383, Total reward: -152.0, Epsilon: 0.146\n",
      "Episode 384, Total reward: -200.0, Epsilon: 0.145\n",
      "Episode 385, Total reward: -188.0, Epsilon: 0.144\n",
      "Episode 386, Total reward: -153.0, Epsilon: 0.144\n",
      "Episode 387, Total reward: -200.0, Epsilon: 0.143\n",
      "Episode 388, Total reward: -171.0, Epsilon: 0.142\n",
      "Episode 389, Total reward: -200.0, Epsilon: 0.142\n",
      "Episode 390, Total reward: -143.0, Epsilon: 0.141\n",
      "Episode 391, Total reward: -171.0, Epsilon: 0.140\n",
      "Episode 392, Total reward: -200.0, Epsilon: 0.139\n",
      "Episode 393, Total reward: -156.0, Epsilon: 0.139\n",
      "Episode 394, Total reward: -193.0, Epsilon: 0.138\n",
      "Episode 395, Total reward: -199.0, Epsilon: 0.137\n",
      "Episode 396, Total reward: -152.0, Epsilon: 0.137\n",
      "Episode 397, Total reward: -193.0, Epsilon: 0.136\n",
      "Episode 398, Total reward: -158.0, Epsilon: 0.135\n",
      "Episode 399, Total reward: -200.0, Epsilon: 0.135\n",
      "Episode 400, Total reward: -200.0, Epsilon: 0.134\n",
      "Episode 401, Total reward: -150.0, Epsilon: 0.133\n",
      "Episode 402, Total reward: -128.0, Epsilon: 0.133\n",
      "Episode 403, Total reward: -200.0, Epsilon: 0.132\n",
      "Episode 404, Total reward: -196.0, Epsilon: 0.131\n",
      "Episode 405, Total reward: -200.0, Epsilon: 0.131\n",
      "Episode 406, Total reward: -191.0, Epsilon: 0.130\n",
      "Episode 407, Total reward: -200.0, Epsilon: 0.129\n",
      "Episode 408, Total reward: -163.0, Epsilon: 0.129\n",
      "Episode 409, Total reward: -152.0, Epsilon: 0.128\n",
      "Episode 410, Total reward: -200.0, Epsilon: 0.127\n",
      "Episode 411, Total reward: -113.0, Epsilon: 0.127\n",
      "Episode 412, Total reward: -200.0, Epsilon: 0.126\n",
      "Episode 413, Total reward: -200.0, Epsilon: 0.126\n",
      "Episode 414, Total reward: -149.0, Epsilon: 0.125\n",
      "Episode 415, Total reward: -111.0, Epsilon: 0.124\n",
      "Episode 416, Total reward: -155.0, Epsilon: 0.124\n",
      "Episode 417, Total reward: -110.0, Epsilon: 0.123\n",
      "Episode 418, Total reward: -182.0, Epsilon: 0.122\n",
      "Episode 419, Total reward: -172.0, Epsilon: 0.122\n",
      "Episode 420, Total reward: -115.0, Epsilon: 0.121\n",
      "Episode 421, Total reward: -200.0, Epsilon: 0.121\n",
      "Episode 422, Total reward: -124.0, Epsilon: 0.120\n",
      "Episode 423, Total reward: -166.0, Epsilon: 0.119\n",
      "Episode 424, Total reward: -121.0, Epsilon: 0.119\n",
      "Episode 425, Total reward: -153.0, Epsilon: 0.118\n",
      "Episode 426, Total reward: -200.0, Epsilon: 0.118\n",
      "Episode 427, Total reward: -175.0, Epsilon: 0.117\n",
      "Episode 428, Total reward: -171.0, Epsilon: 0.116\n",
      "Episode 429, Total reward: -137.0, Epsilon: 0.116\n",
      "Episode 430, Total reward: -169.0, Epsilon: 0.115\n",
      "Episode 431, Total reward: -200.0, Epsilon: 0.115\n",
      "Episode 432, Total reward: -181.0, Epsilon: 0.114\n",
      "Episode 433, Total reward: -153.0, Epsilon: 0.114\n",
      "Episode 434, Total reward: -144.0, Epsilon: 0.113\n",
      "Episode 435, Total reward: -200.0, Epsilon: 0.112\n",
      "Episode 436, Total reward: -117.0, Epsilon: 0.112\n",
      "Episode 437, Total reward: -159.0, Epsilon: 0.111\n",
      "Episode 438, Total reward: -127.0, Epsilon: 0.111\n",
      "Episode 439, Total reward: -121.0, Epsilon: 0.110\n",
      "Episode 440, Total reward: -154.0, Epsilon: 0.110\n",
      "Episode 441, Total reward: -189.0, Epsilon: 0.109\n",
      "Episode 442, Total reward: -156.0, Epsilon: 0.109\n",
      "Episode 443, Total reward: -126.0, Epsilon: 0.108\n",
      "Episode 444, Total reward: -159.0, Epsilon: 0.107\n",
      "Episode 445, Total reward: -154.0, Epsilon: 0.107\n",
      "Episode 446, Total reward: -198.0, Epsilon: 0.106\n",
      "Episode 447, Total reward: -138.0, Epsilon: 0.106\n",
      "Episode 448, Total reward: -113.0, Epsilon: 0.105\n",
      "Episode 449, Total reward: -125.0, Epsilon: 0.105\n",
      "Episode 450, Total reward: -156.0, Epsilon: 0.104\n",
      "Episode 451, Total reward: -186.0, Epsilon: 0.104\n",
      "Episode 452, Total reward: -169.0, Epsilon: 0.103\n",
      "Episode 453, Total reward: -101.0, Epsilon: 0.103\n",
      "Episode 454, Total reward: -117.0, Epsilon: 0.102\n",
      "Episode 455, Total reward: -170.0, Epsilon: 0.102\n",
      "Episode 456, Total reward: -161.0, Epsilon: 0.101\n",
      "Episode 457, Total reward: -200.0, Epsilon: 0.101\n",
      "Episode 458, Total reward: -158.0, Epsilon: 0.100\n",
      "Episode 459, Total reward: -194.0, Epsilon: 0.100\n",
      "Episode 460, Total reward: -135.0, Epsilon: 0.099\n",
      "Episode 461, Total reward: -161.0, Epsilon: 0.099\n",
      "Episode 462, Total reward: -168.0, Epsilon: 0.098\n",
      "Episode 463, Total reward: -200.0, Epsilon: 0.098\n",
      "Episode 464, Total reward: -184.0, Epsilon: 0.097\n",
      "Episode 465, Total reward: -198.0, Epsilon: 0.097\n",
      "Episode 466, Total reward: -160.0, Epsilon: 0.096\n",
      "Episode 467, Total reward: -185.0, Epsilon: 0.096\n",
      "Episode 468, Total reward: -159.0, Epsilon: 0.095\n",
      "Episode 469, Total reward: -177.0, Epsilon: 0.095\n",
      "Episode 470, Total reward: -159.0, Epsilon: 0.094\n",
      "Episode 471, Total reward: -152.0, Epsilon: 0.094\n",
      "Episode 472, Total reward: -192.0, Epsilon: 0.093\n",
      "Episode 473, Total reward: -168.0, Epsilon: 0.093\n",
      "Episode 474, Total reward: -173.0, Epsilon: 0.092\n",
      "Episode 475, Total reward: -164.0, Epsilon: 0.092\n",
      "Episode 476, Total reward: -169.0, Epsilon: 0.092\n",
      "Episode 477, Total reward: -145.0, Epsilon: 0.091\n",
      "Episode 478, Total reward: -143.0, Epsilon: 0.091\n",
      "Episode 479, Total reward: -154.0, Epsilon: 0.090\n",
      "Episode 480, Total reward: -188.0, Epsilon: 0.090\n",
      "Episode 481, Total reward: -148.0, Epsilon: 0.089\n",
      "Episode 482, Total reward: -183.0, Epsilon: 0.089\n",
      "Episode 483, Total reward: -163.0, Epsilon: 0.088\n",
      "Episode 484, Total reward: -150.0, Epsilon: 0.088\n",
      "Episode 485, Total reward: -151.0, Epsilon: 0.088\n",
      "Episode 486, Total reward: -200.0, Epsilon: 0.087\n",
      "Episode 487, Total reward: -122.0, Epsilon: 0.087\n",
      "Episode 488, Total reward: -147.0, Epsilon: 0.086\n",
      "Episode 489, Total reward: -148.0, Epsilon: 0.086\n",
      "Episode 490, Total reward: -141.0, Epsilon: 0.085\n",
      "Episode 491, Total reward: -184.0, Epsilon: 0.085\n",
      "Episode 492, Total reward: -115.0, Epsilon: 0.084\n",
      "Episode 493, Total reward: -139.0, Epsilon: 0.084\n",
      "Episode 494, Total reward: -130.0, Epsilon: 0.084\n",
      "Episode 495, Total reward: -128.0, Epsilon: 0.083\n",
      "Episode 496, Total reward: -198.0, Epsilon: 0.083\n",
      "Episode 497, Total reward: -200.0, Epsilon: 0.082\n",
      "Episode 498, Total reward: -111.0, Epsilon: 0.082\n",
      "Episode 499, Total reward: -158.0, Epsilon: 0.082\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Hyperparameter Sensitivity (Learning Rate)",
   "id": "9318bb505d89db82"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-19T10:54:37.804517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lrs = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "mc_results_lr = {}\n",
    "\n",
    "for lr in lrs:\n",
    "    all_train_rewards = []\n",
    "    all_eval_rewards = []\n",
    "    for seed in SEEDS:\n",
    "        print(\"lr = \", lr, \"seed = \", seed)\n",
    "        set_seed(seed)\n",
    "        env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "        dqn_agent = DQNAgent(\n",
    "            env=env,\n",
    "            gamma=0.99,\n",
    "            lr=1e-3,\n",
    "            batch_size=64,\n",
    "            epsilon=1.0,\n",
    "            epsilon_decay=0.995,\n",
    "            epsilon_min=0.01,\n",
    "            hidden_dim=128,\n",
    "            tau=1.0,\n",
    "            num_episodes=1000,\n",
    "            eval_interval=5,\n",
    "            target_update_freq=10,\n",
    "            constant_epsilon=False\n",
    "        )\n",
    "\n",
    "        rewards_history, eval_rewards_history = dqn_agent.train_with_seed()\n",
    "\n",
    "        all_train_rewards.append(rewards_history)\n",
    "        all_eval_rewards.append(eval_rewards_history)\n",
    "\n",
    "    all_train_rewards = np.array(all_train_rewards)\n",
    "    avg_train_rewards = all_train_rewards.mean(axis=0)\n",
    "\n",
    "    all_eval_rewards = np.array(all_eval_rewards)\n",
    "    avg_eval_rewards = all_eval_rewards.mean(axis=0)\n",
    "\n",
    "    mc_results_lr[f\"LR={lr}\"] = {\n",
    "        \"params\": {\n",
    "            \"lr\": lr,\n",
    "            \"gamma\": 0.99,\n",
    "            \"batch_size\": 64,\n",
    "            \"epsilon_decay\": 0.99,\n",
    "            \"target_update_freq\": 10\n",
    "        },\n",
    "        \"avg_train_rewards\": avg_train_rewards,\n",
    "        \"avg_eval_rewards\": avg_eval_rewards\n",
    "    }"
   ],
   "id": "dc24fd971caf7ada",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr =  0.005 seed =  42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/farida/PycharmProjects/RL_Project/Agents/DQNAgent.py:66: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  next_state = torch.FloatTensor(transitions.next_state)  # shape: [batch_size, state_dim]\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot results\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "for lr, result in mc_results_lr.items():\n",
    "    plt.plot(result[\"avg_train_rewards\"], label=lr)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average Reward (3 seeds)\")\n",
    "plt.title(\"DQN Learning Rate Comparison on Mountain Car\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "76c9b46ad539f712",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "window_size = 20  # Smooth over 20 episodes (adjust as needed)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for lr, result in mc_results_lr.items():\n",
    "    rewards = result[\"avg_train_rewards\"]\n",
    "    # Compute moving average\n",
    "    smoothed_rewards = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "    episodes = np.arange(len(smoothed_rewards))\n",
    "    plt.plot(episodes, smoothed_rewards, label=f\"{lr}\")\n",
    "\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(f\"Smoothed Reward (Window={window_size})\")\n",
    "plt.title(\"DQN Learning Rate Comparison (Moving Average)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "b476c52e508c76f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plot results\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "for lr, result in mc_results_lr.items():\n",
    "    plt.plot(result[\"avg_eval_rewards\"], label=lr)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average Reward (3 seeds)\")\n",
    "plt.title(\"DQN Learning Rate Comparison on Mountain Car\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "1101700591be4e47"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "window_size = 20  # Smooth over 20 episodes (adjust as needed)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for lr, result in mc_results_lr.items():\n",
    "    rewards = result[\"avg_eval_rewards\"]\n",
    "    # Compute moving average\n",
    "    smoothed_rewards = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "    episodes = np.arange(len(smoothed_rewards))\n",
    "    plt.plot(episodes, smoothed_rewards, label=f\"{lr}\")\n",
    "\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(f\"Smoothed Reward (Window={window_size})\")\n",
    "plt.title(\"DQN Learning Rate Comparison (Moving Average)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "efce60f0095b5506"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Hyperparameter Sensitivity (Epsilon Decay)",
   "id": "c9cf1799d754dcf5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "eps_decay_list = [0.99, 0.995, 0.999]\n",
    "mc_results_eps_decay = {}\n",
    "\n",
    "for eps_decay in eps_decay_list:\n",
    "    all_rewards = []\n",
    "    for seed in SEEDS:\n",
    "        print(\"eps_decay = \", eps_decay, \"seed = \", seed)\n",
    "        set_seed(seed)\n",
    "        env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "        agent, rewards = DQNAgent.train_dqn(\n",
    "            env=env,\n",
    "            gamma=0.99,\n",
    "            batch_size=64,\n",
    "            epsilon=1.0,\n",
    "            epsilon_decay=eps_decay,\n",
    "            use_target_net=True,\n",
    "            constant_epsilon=False,\n",
    "            num_episodes=1500,\n",
    "            target_update_freq=10,\n",
    "            hidden_dim=128,\n",
    "            lr=1e-3,\n",
    "            seed=seed\n",
    "        )\n",
    "\n",
    "        all_rewards.append(rewards)\n",
    "\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    avg_rewards = all_rewards.mean(axis=0)\n",
    "    mc_results_eps_decay[f\"eps_decay={eps_decay}\"] = {\n",
    "        \"params\": {\n",
    "            \"lr\": 1e-3,\n",
    "            \"gamma\": 0.99,\n",
    "            \"batch_size\": 32,\n",
    "            \"epsilon_decay\": eps_decay,\n",
    "            \"target_update_freq\": 10\n",
    "        },\n",
    "        \"avg_rewards\": avg_rewards\n",
    "    }"
   ],
   "id": "557ec35d63112d56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot results\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "for eps_decay, result in mc_results_eps_decay.items():\n",
    "    plt.plot(result[\"avg_rewards\"], label=eps_decay)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average Reward (3 seeds)\")\n",
    "plt.title(\"DQN Learning Rate Comparison on MountainCar-v0\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "34aa6a3aae2f6df4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "window_size = 20  # Smooth over 20 episodes (adjust as needed)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for eps_decay, result in mc_results_eps_decay.items():\n",
    "    rewards = result[\"avg_rewards\"]\n",
    "    # Compute moving average\n",
    "    smoothed_rewards = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "    episodes = np.arange(len(smoothed_rewards))\n",
    "    plt.plot(episodes, smoothed_rewards, label=f\"{eps_decay}\")\n",
    "\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(f\"Smoothed Reward (Window={window_size})\")\n",
    "plt.title(\"DQN Learning Rate Comparison (Moving Average)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "b63f2acebd26dcef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Hyperparameter Sensitivity (Batch Size)",
   "id": "6e5298cd00c72780"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "batch_sized = [64, 32, 16]\n",
    "mc_results_bs = {}\n",
    "\n",
    "for bs in batch_sized:\n",
    "    all_rewards = []\n",
    "    for seed in SEEDS:\n",
    "        print(\"bs = \", bs, \"seed = \", seed)\n",
    "        set_seed(seed)\n",
    "        env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "        agent, rewards = DQNAgent.train_dqn(\n",
    "            env=env,\n",
    "            gamma=0.99,\n",
    "            batch_size=bs,\n",
    "            epsilon=1.0,\n",
    "            epsilon_decay=0.99,\n",
    "            use_target_net=True,\n",
    "            constant_epsilon=False,\n",
    "            num_episodes=1500,\n",
    "            target_update_freq=10,\n",
    "            lr=1e-3,\n",
    "            seed=seed\n",
    "        )\n",
    "\n",
    "        all_rewards.append(rewards)\n",
    "\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    avg_rewards = all_rewards.mean(axis=0)\n",
    "    acrobot_results_bs[f\"Batch Size={bs}\"] = {\n",
    "        \"params\": {\n",
    "            \"lr\": 1e-3,\n",
    "            \"gamma\": 0.99,\n",
    "            \"batch_size\": bs,\n",
    "            \"epsilon_decay\": 0.99,\n",
    "            \"target_update_freq\": 10\n",
    "        },\n",
    "        \"avg_rewards\": avg_rewards\n",
    "    }"
   ],
   "id": "1b0c938e0ee85b06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot results\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "for bs, result in mc_results_bs.items():\n",
    "    plt.plot(result[\"avg_rewards\"], label=bs)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average Reward (3 seeds)\")\n",
    "plt.title(\"DQN Learning Rate Comparison on Mountain Car\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "e69b292379dc4fac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "window_size = 20  # Smooth over 20 episodes (adjust as needed)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for batch_size, result in mc_results_bs.items():\n",
    "    rewards = result[\"avg_rewards\"]\n",
    "    # Compute moving average\n",
    "    smoothed_rewards = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "    episodes = np.arange(len(smoothed_rewards))\n",
    "    plt.plot(episodes, smoothed_rewards, label=f\"{batch_size}\")\n",
    "\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(f\"Smoothed Reward (Window={window_size})\")\n",
    "plt.title(\"DQN Learning Rate Comparison (Moving Average)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "acd36709d40357f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Hyperparameter Sensitivity (Target Net Update Freq)",
   "id": "7c6c2d081841f344"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "update_freq = [1, 10, 20, 100]\n",
    "mc_results_tf = {}\n",
    "\n",
    "for bs in update_freq:\n",
    "    all_rewards = []\n",
    "    for seed in SEEDS:\n",
    "        print(\"bs = \", bs, \"seed = \", seed)\n",
    "        set_seed(seed)\n",
    "        env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "        agent, rewards = DQNAgent.train_dqn(\n",
    "            env=env,\n",
    "            gamma=0.99,\n",
    "            batch_size=32,\n",
    "            epsilon=1.0,\n",
    "            epsilon_decay=0.99,\n",
    "            use_target_net=True,\n",
    "            constant_epsilon=False,\n",
    "            num_episodes=1500,\n",
    "            target_update_freq=bs,\n",
    "            lr=1e-3,\n",
    "            seed=seed\n",
    "        )\n",
    "\n",
    "        all_rewards.append(rewards)\n",
    "\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    avg_rewards = all_rewards.mean(axis=0)\n",
    "    mc_results_tf[f\"Batch Size={bs}\"] = {\n",
    "        \"params\": {\n",
    "            \"lr\": 1e-3,\n",
    "            \"gamma\": 0.99,\n",
    "            \"batch_size\": 32,\n",
    "            \"epsilon_decay\": 0.99,\n",
    "            \"target_update_freq\": bs\n",
    "        },\n",
    "        \"avg_rewards\": avg_rewards\n",
    "    }"
   ],
   "id": "e4165427885a77e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot results\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "for bs, result in mc_results_tf.items():\n",
    "    plt.plot(result[\"avg_rewards\"], label=bs)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average Reward (3 seeds)\")\n",
    "plt.title(\"DQN Learning Rate Comparison on Mountain Car\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "d299ea01c7cffd21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "window_size = 20  # Smooth over 20 episodes (adjust as needed)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for batch_size, result in mc_results_tf.items():\n",
    "    rewards = result[\"avg_rewards\"]\n",
    "    # Compute moving average\n",
    "    smoothed_rewards = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "    episodes = np.arange(len(smoothed_rewards))\n",
    "    plt.plot(episodes, smoothed_rewards, label=f\"{batch_size}\")\n",
    "\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(f\"Smoothed Reward (Window={window_size})\")\n",
    "plt.title(\"DQN Learning Rate Comparison (Moving Average)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "774b47eeb6032fbc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "95edb188cd7ee1c7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
